{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_channels=128, n_filters=128, apply_bn = False, apply_res=True):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.apply_bn = apply_bn\n",
    "        self.apply_res = apply_res\n",
    "        kernel_size=3\n",
    "        p=1\n",
    "        \n",
    "        self.first_conv = nn.Conv2d(input_channels, n_filters, kernel_size, padding=p)\n",
    "        self.first_activate = nn.ReLU(inplace=True)\n",
    "        if apply_bn is True:\n",
    "            self.first_bn = nn.BatchNorm2d(n_filters)\n",
    "        \n",
    "        self.second_conv = nn.Conv2d(n_filters, n_filters, kernel_size, padding=p)\n",
    "        self.second_activate = nn.ReLU(inplace=True)\n",
    "        if apply_bn is True:\n",
    "            self.second_bn = nn.BatchNorm2d(n_filters)\n",
    "        self.AvgPool = nn.AvgPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_op = x\n",
    "        \n",
    "        x = self.first_conv(x)\n",
    "        if self.apply_bn:\n",
    "            x = self.first_bn(x)\n",
    "        x = self.first_activate(x)\n",
    "        \n",
    "        x = self.second_conv(x)\n",
    "       \n",
    "        if self.apply_res:\n",
    "            x  = x + first_op\n",
    "        if self.apply_bn:\n",
    "            x = self.second_bn(x)\n",
    "        x = self.second_activate(x)\n",
    "        skip = x\n",
    "        x = self.AvgPool(x)\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_filters=128, apply_bn = False, dp=False):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.apply_bn = apply_bn\n",
    "        self.dp = dp\n",
    "        kernel_size=3 \n",
    "        p=1\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.first_conv = nn.Conv2d(n_filters*2, n_filters, kernel_size, padding=p)\n",
    "        if apply_bn:\n",
    "            self.first_bn = nn.BatchNorm2d(n_filters)\n",
    "        self.first_activate = nn.ReLU(inplace=True)\n",
    "        self.second_conv = nn.Conv2d(n_filters, n_filters, kernel_size, padding=p)\n",
    "        if apply_bn:\n",
    "            self.second_bn = nn.BatchNorm2d(n_filters)\n",
    "        self.second_activate = nn.ReLU(inplace=True)\n",
    "        if dp is not False:\n",
    "            self.dp = nn.Dropout(p=dp, inplace=True)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x  = torch.cat((x, skip), dim=1)\n",
    "        \n",
    "        if self.dp is not False:\n",
    "            x = self.dp(x)\n",
    "        \n",
    "        x = self.first_conv(x)\n",
    "        if self.apply_bn:\n",
    "            x = self.first_bn(x)\n",
    "        x = self.first_activate(x)\n",
    "        \n",
    "        \n",
    "        x = self.second_conv(x)\n",
    "        if self.apply_bn:\n",
    "            x = self.second_bn(x)\n",
    "        x = self.second_activate(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DilatedBlock(nn.Module):\n",
    "    def __init__(self, n_channels=128, dilation_rates=[1, 2, 4, 8], number_blocks=4, apply_bn=False, dp=False):\n",
    "        super(DilatedBlock, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        self.apply_bn = apply_bn\n",
    "        self.dp = dp\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels, padding=dilation_rates[0], kernel_size=kernel_size, dilation=dilation_rates[0])\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, padding=dilation_rates[1], kernel_size=kernel_size, dilation=dilation_rates[1])\n",
    "        self.conv3 = nn.Conv2d(n_channels, n_channels, padding=dilation_rates[2], kernel_size=kernel_size, dilation=dilation_rates[2])\n",
    "        self.conv4 = nn.Conv2d(n_channels, n_channels, padding=dilation_rates[3], kernel_size=kernel_size, dilation=dilation_rates[3])\n",
    "        if apply_bn:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm2d(n_channels)]*4)\n",
    "        self.activates = nn.ModuleList([nn.ReLU(inplace=True)]*4)\n",
    "        if dp is not False:\n",
    "            self.dps = nn.ModuleList([nn.Dropout(p=dp, inplace=True)]*4)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        dilate1 = x\n",
    "        if self.apply_bn :\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activates[0](x)\n",
    "        if self.dp is not False:\n",
    "            x = self.dps[0](x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        dilate2 = x\n",
    "        if self.apply_bn :\n",
    "            x = self.bns[1](x)\n",
    "        x = self.activates[1](x)\n",
    "        if self.dp is not False:\n",
    "            x = self.dps[1](x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        dilate3 = x\n",
    "        if self.apply_bn :\n",
    "            x = self.bns[2](x)\n",
    "        x = self.activates[2](x)\n",
    "        if self.dp is not False:\n",
    "            x = self.dps[2](x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        dilate4 = x\n",
    "        if self.apply_bn :\n",
    "            x = self.bns[3](x)\n",
    "        x = self.activates[3](x)\n",
    "        if self.dp is not False:\n",
    "            x = self.dps[3](x)\n",
    "        \n",
    "        dilate = dilate1 + dilate2 + dilate3 + dilate4\n",
    "        return dilate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDR(nn.Module):\n",
    "    def __init__(self, n_channels=128, apply_bn=True, dropout=0.1):\n",
    "        super(UNetDR, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.EB1 = EncoderBlock(1, n_channels, apply_bn=apply_bn, apply_res=False)\n",
    "        #self.AvgPools = nn.ModuleList([nn.AvgPool2d(2)]*5)\n",
    "        self.EB2 = EncoderBlock(n_channels, n_channels, apply_bn=apply_bn)\n",
    "        self.EB3 = EncoderBlock(n_channels, n_channels, apply_bn=apply_bn)\n",
    "        self.EB4 = EncoderBlock(n_channels, n_channels, apply_bn=apply_bn)\n",
    "        self.dp4 = nn.Dropout(p=dropout, inplace=True)\n",
    "        self.EB5 = EncoderBlock(n_channels, n_channels, apply_bn=apply_bn)\n",
    "        self.dp5 = nn.Dropout(p=dropout, inplace=True)\n",
    "        self.DB5 = DecoderBlock(n_channels, apply_bn=apply_bn, dp=dropout)\n",
    "        self.DB4 = DecoderBlock(n_channels, apply_bn=apply_bn, dp=dropout)\n",
    "        self.DB3 = DecoderBlock(n_channels, apply_bn=apply_bn, dp=False)\n",
    "        self.DB2 = DecoderBlock(n_channels, apply_bn=apply_bn, dp=False)\n",
    "        self.DB1 = DecoderBlock(n_channels, apply_bn=apply_bn, dp=False)\n",
    "        self.last_conv = nn.Conv2d(n_channels, 1, 3, padding=1)\n",
    "        self.DB = DilatedBlock(n_channels, apply_bn=apply_bn, dp=dropout)\n",
    "        self.initialize_weights()\n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.EB1(x)\n",
    "        x, skip2 = self.EB2(x)\n",
    "        x, skip3 = self.EB3(x)\n",
    "        x, skip4 = self.EB4(x)\n",
    "        x = self.dp4(x)\n",
    "        x, skip5 = self.EB5(x)\n",
    "        x = self.dp5(x)\n",
    "        x = self.DB(x)\n",
    "        x = self.DB5(x, skip5)\n",
    "        x = self.DB4(x, skip4)\n",
    "        x = self.DB3(x, skip3)\n",
    "        x = self.DB2(x, skip2)\n",
    "        x = self.DB1(x, skip1)\n",
    "        x = self.last_conv(x)\n",
    "        return x\n",
    "    def initialize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            for m in self.modules():\n",
    "                classname = m.__class__.__name__\n",
    "                if classname.find('Conv2d') != -1:\n",
    "                    torch.nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.fill_(0)\n",
    "            torch.nn.init.normal_(self.last_conv.weight.data, std=0.001)\n",
    "            self.last_conv.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNetDR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.from_numpy(np.random.rand(1, 1, 512, 512).astype(np.float32))\n",
    "x = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
